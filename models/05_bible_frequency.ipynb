{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType, DataType,ArrayType\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.ml import Pipeline\n",
    "from IPython.display import display, HTML\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 14:08:07 WARN Utils: Your hostname, Glorias-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.39 instead (on interface en0)\n",
      "22/11/21 14:08:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/gloria/opt/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/gloria/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/gloria/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4d0921f2-e2a3-4bf4-90d4-fdb281b08878;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.2.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 in central\n",
      "\tfound org.postgresql#postgresql;42.5.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 758ms :: artifacts dl 80ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.2.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   19  |   0   |   0   |   0   ||   19  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4d0921f2-e2a3-4bf4-90d4-fdb281b08878\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 19 already retrieved (0kB/32ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 14:08:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 14:08:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Spark NLP version 4.2.0\n",
      "Apache Spark version: 3.3.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.39:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc4e05eeb20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# spark = sparknlp.start() # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[2]\")\\\n",
    "    .config(\"spark.driver.memory\",\"8G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.3,org.postgresql:postgresql:42.5.0\")\\\n",
    "    .getOrCreate()\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            document|highlighted_keywords|            keywords|            sentence|                text|               token|     unique_keywords|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[{document, 0, []...|In the <span styl...|[{chunk, 7, [], 1...|[{document, 0, []...|In the beginning ...|[{token, 0, [], 1...|[beginning, god, ...|\n",
      "|[{document, 0, []...|Now the <span sty...|[{chunk, 8, [], 1...|[{document, 0, []...|Now the earth was...|[{token, 0, [], 2...|[earth, formless,...|\n",
      "|[{document, 0, []...|And <span style=\"...|[{chunk, 4, [], 6...|[{document, 0, []...|And God said, \"Le...|[{token, 0, [], 2...|[god, said, light...|\n",
      "|[{document, 0, []...|<span style=\"back...|[{chunk, 0, [], 2...|[{document, 0, []...|God saw that the ...|[{token, 0, [], 2...|[god, saw, light,...|\n",
      "|[{document, 0, []...|<span style=\"back...|[{chunk, 0, [], 2...|[{document, 0, []...|God called the li...|[{token, 0, [], 2...|[god, called, lig...|\n",
      "|[{document, 0, []...|And <span style=\"...|[{chunk, 4, [], 6...|[{document, 0, []...|And God said, \"Le...|[{token, 0, [], 2...|[god, said, expan...|\n",
      "|[{document, 0, []...|So <span style=\"b...|[{chunk, 3, [], 5...|[{document, 0, []...|So God made the e...|[{token, 0, [], 1...|[god, made, expan...|\n",
      "|[{document, 0, []...|<span style=\"back...|[{chunk, 0, [], 2...|[{document, 0, []...|God called the ex...|[{token, 0, [], 2...|[god, called, exp...|\n",
      "|[{document, 0, []...|And <span style=\"...|[{chunk, 4, [], 6...|[{document, 0, []...|And God said, \"Le...|[{token, 0, [], 2...|[god, said, water...|\n",
      "|[{document, 0, []...|<span style=\"back...|[{chunk, 0, [], 2...|[{document, 0, []...|God called the dr...|[{token, 0, [], 2...|[god, called, dry...|\n",
      "|[{document, 0, []...|Then <span style=...|[{chunk, 5, [], 7...|[{document, 0, []...|Then God said, \"L...|[{token, 0, [], 3...|[god, said, land,...|\n",
      "|[{document, 0, []...|The <span style=\"...|[{chunk, 4, [], 7...|[{document, 0, []...|The land produced...|[{token, 0, [], 2...|[land, produced, ...|\n",
      "|[{document, 0, []...|And there was <sp...|[{chunk, 14, [], ...|[{document, 0, []...|And there was eve...|[{token, 0, [], 2...|[evening, third, ...|\n",
      "|[{document, 0, []...|And <span style=\"...|[{chunk, 4, [], 6...|[{document, 0, []...|And God said, \"Le...|[{token, 0, [], 2...|[god, said, light...|\n",
      "|[{document, 0, []...|and <span style=\"...|[{chunk, 4, [], 6...|[{document, 0, []...|and let them be l...|[{token, 0, [], 2...|[let, lights, exp...|\n",
      "|[{document, 0, []...|<span style=\"back...|[{chunk, 0, [], 2...|[{document, 0, []...|God made two grea...|[{token, 0, [], 2...|[god, made, two, ...|\n",
      "|[{document, 0, []...|<span style=\"back...|[{chunk, 0, [], 2...|[{document, 0, []...|God set them in t...|[{token, 0, [], 2...|[god, set, expans...|\n",
      "|[{document, 0, []...|to <span style=\"b...|[{chunk, 3, [], 8...|[{document, 0, []...|to govern the day...|[{token, 0, [], 1...|[govern, day, nig...|\n",
      "|[{document, 0, []...|And there was <sp...|[{chunk, 14, [], ...|[{document, 0, []...|And there was eve...|[{token, 0, [], 2...|[evening, fourth,...|\n",
      "|[{document, 0, []...|And <span style=\"...|[{chunk, 4, [], 6...|[{document, 0, []...|And God said, \"Le...|[{token, 0, [], 2...|[god, said, water...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                text|            keywords|\n",
      "+--------------------+--------------------+\n",
      "|In the beginning ...|[beginning, god, ...|\n",
      "|Now the earth was...|[earth, formless,...|\n",
      "|And God said, \"Le...|[god, said, light...|\n",
      "|God saw that the ...|[god, saw, light,...|\n",
      "|God called the li...|[god, called, lig...|\n",
      "|And God said, \"Le...|[god, said, expan...|\n",
      "|So God made the e...|[god, made, expan...|\n",
      "|God called the ex...|[god, called, exp...|\n",
      "|And God said, \"Le...|[god, said, water...|\n",
      "|God called the dr...|[god, called, dry...|\n",
      "|Then God said, \"L...|[god, said, land,...|\n",
      "|The land produced...|[land, produced, ...|\n",
      "|And there was eve...|[evening, third, ...|\n",
      "|And God said, \"Le...|[god, said, light...|\n",
      "|and let them be l...|[let, lights, exp...|\n",
      "|God made two grea...|[god, made, two, ...|\n",
      "|God set them in t...|[god, set, expans...|\n",
      "|to govern the day...|[govern, day, nig...|\n",
      "|And there was eve...|[evening, fourth,...|\n",
      "|And God said, \"Le...|[god, said, water...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### bible keyword frequency\n",
    "df_keyword=spark.read.json('./bibleKeyword.json/')          \n",
    "df_keyword.show()\n",
    "df_keyword=df_keyword.selectExpr('text','keywords.result as keywords')                  \n",
    "df_keyword.show()                                                              \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- keyword: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                text|             keyword|\n",
      "+--------------------+--------------------+\n",
      "|In the beginning ...|           beginning|\n",
      "|In the beginning ...|                 god|\n",
      "|In the beginning ...|             created|\n",
      "|In the beginning ...|             heavens|\n",
      "|In the beginning ...|               earth|\n",
      "|In the beginning ...|       beginning god|\n",
      "|In the beginning ...|         god created|\n",
      "|In the beginning ...|beginning god cre...|\n",
      "|In the beginning ...| created the heavens|\n",
      "|Now the earth was...|               earth|\n",
      "|Now the earth was...|            formless|\n",
      "|Now the earth was...|               empty|\n",
      "|Now the earth was...|            darkness|\n",
      "|Now the earth was...|             surface|\n",
      "|Now the earth was...|                deep|\n",
      "|Now the earth was...|              spirit|\n",
      "|Now the earth was...|                 god|\n",
      "|Now the earth was...|            hovering|\n",
      "|Now the earth was...|              waters|\n",
      "|Now the earth was...|  earth was formless|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df_keyword.select('text',explode(df_keyword.keywords).alias('keyword'))\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, keyword: string, count: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()\n",
    "df_keyword=df2.select('keyword').groupBy('keyword').count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+\n",
      "|summary|keyword|             count|\n",
      "+-------+-------+------------------+\n",
      "|  count|  96024|             96024|\n",
      "|   mean|   null|4.4531263017578935|\n",
      "| stddev|   null| 42.76291857387599|\n",
      "|    min|  aaron|                 1|\n",
      "|    max|zuzites|              7144|\n",
      "+-------+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_keyword.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|keyword|count|\n",
      "+-------+-----+\n",
      "|   lord| 7144|\n",
      "|    god| 3691|\n",
      "|   said| 3019|\n",
      "|    one| 2410|\n",
      "|   king| 2226|\n",
      "|    son| 2170|\n",
      "| people| 2064|\n",
      "|    man| 1987|\n",
      "| israel| 1701|\n",
      "|    men| 1687|\n",
      "|   like| 1503|\n",
      "|   land| 1362|\n",
      "|    day| 1308|\n",
      "|   come| 1286|\n",
      "|     us| 1279|\n",
      "|  jesus| 1217|\n",
      "|   went| 1213|\n",
      "|   came| 1173|\n",
      "|     go| 1153|\n",
      "|    may| 1131|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_keyword.orderBy('count',ascending=False).show()\n",
    "# df_keyword.printSchema()\n",
    "# df_keyword.createTempView('keyword')     \n",
    "# df_keyword.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_keyword.write.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://192.168.1.39:5432/aiknowledge\")\\\n",
    "    .option(\"dbtable\", \"public.bible_keyword\")\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"postgres\")\\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write to neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>n</th><th>n_sq</th></tr><tr><td style=\"text-align:right\">1</td><td style=\"text-align:right\">1</td></tr><tr><td style=\"text-align:right\">2</td><td style=\"text-align:right\">4</td></tr><tr><td style=\"text-align:right\">3</td><td style=\"text-align:right\">9</td></tr></table>"
      ],
      "text/plain": [
       " n | n_sq \n",
       "---|------\n",
       " 1 |    1 \n",
       " 2 |    4 \n",
       " 3 |    9 "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py2neo import Graph\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "graph = Graph(\"neo4j://localhost:7687\", auth=(\"daniel\", \"fighting\"),name=\"aiknowledge\")\n",
    "graph.run(\"UNWIND range(1, 3) AS n RETURN n, n * n as n_sq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|           keyword|count|\n",
      "+------------------+-----+\n",
      "|            waters|  157|\n",
      "|         first day|   48|\n",
      "|  noah and entered|    2|\n",
      "|        great deep|    4|\n",
      "|             still|  278|\n",
      "|        may canaan|    2|\n",
      "|          ashkenaz|    3|\n",
      "|men moved eastward|    1|\n",
      "|             serug|    6|\n",
      "|            spared|   24|\n",
      "| anything too hard|    2|\n",
      "|    sake of twenty|    1|\n",
      "|      two men said|    1|\n",
      "|        swept away|   10|\n",
      "|         ammonites|   84|\n",
      "|              hazo|    1|\n",
      "|       master made|    1|\n",
      "|           jewelry|   12|\n",
      "|         go toward|    1|\n",
      "|        loved esau|    1|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_keyword.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96024/96024 [12:08<00:00, 131.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "for row in tqdm(df_keyword.rdd.collect()):\n",
    "    keyword = Node('bible_keyword', name=row['keyword'])\n",
    "    keyword['count']=row['count']\n",
    "    graph.create(keyword)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f70b66c476b517a3398e503e066261bd5effd8e36d6bd83ebcc33f32092e45d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
