{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3c755e-802b-4d96-93e3-6f0013f434af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.39:4043\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1667899230056)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "java.lang.ClassNotFoundException",
     "evalue": " org.postgresql.Driver",
     "output_type": "error",
     "traceback": [
      "java.lang.ClassNotFoundException: org.postgresql.Driver",
      "  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:72)",
      "  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)",
      "  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)",
      "  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "val jdbcDF = spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://127.0.0.1:5432/haystack\")\n",
    "  .option(\"dbtable\", \"public.document\")\n",
    "  .option(\"user\", \"postgres\")\n",
    "  .option(\"password\", \"postgres\")\n",
    "  .option(\"driver\", \"org.postgresql.Driver\")                                     \n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52bb04c-b388-4ab0-a248-0bb4bcd252ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
